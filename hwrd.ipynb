{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fc33be-5bba-4bd2-b348-7e886452e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbe8048-c2d7-40a0-a33e-6a672cc60f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0acc31-5b77-4433-babd-47f341550916",
   "metadata": {},
   "source": [
    "fetch, clean up and separate datasets into training, validation, and testing sets at a ratio of 1:1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fa575e-fd26-4c91-a22f-05dc8d51b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = fetch_ucirepo(id=887)\n",
    "X = dataset1.data.features \n",
    "y = dataset1.data.targets\n",
    "df1 = pd.concat([X,y], axis = 1)\n",
    "df1 = df1.dropna()\n",
    "df1 = df1.drop_duplicates()\n",
    "temp = np.array_split(df1, 3)\n",
    "df1_train = temp[2]\n",
    "df1_validate = temp[1]\n",
    "df1_test = temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c773db84-7522-48d6-9576-fdc0052d34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = fetch_ucirepo(id=15)\n",
    "X2 = dataset2.data.features\n",
    "y2 = dataset2.data.targets\n",
    "df2 = pd.concat([X2,y2], axis = 1)\n",
    "df2 = df2.drop_duplicates()\n",
    "df2 = df2.dropna()\n",
    "temp2 = np.array_split(df2, 3)\n",
    "df2_train = temp2[0]\n",
    "df2_validate = temp2[1]\n",
    "df2_test = temp2[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addd85f-a4a7-410f-aeb0-2696d8c9874f",
   "metadata": {},
   "source": [
    "get a list of feature names for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd12644-4323-4394-95fd-275746a2119d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = df1.columns.tolist()\n",
    "column_list = feature_list[:]\n",
    "feature_list.pop()\n",
    "\n",
    "feature_list_2 = df2.columns.tolist()\n",
    "column_list_2 = feature_list_2[:]\n",
    "feature_list_2.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec4f1279-9270-4ad7-a871-06d899955ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RIAGENDR', 'PAQ605', 'BMXBMI', 'LBXGLU', 'DIQ010', 'LBXGLT', 'LBXIN'] ['Clump_thickness', 'Uniformity_of_cell_size', 'Uniformity_of_cell_shape', 'Marginal_adhesion', 'Single_epithelial_cell_size', 'Bare_nuclei', 'Bland_chromatin', 'Normal_nucleoli', 'Mitoses']\n"
     ]
    }
   ],
   "source": [
    "print(feature_list, feature_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa011c0a-ee83-40a0-b935-0607632255fc",
   "metadata": {},
   "source": [
    "define DTNode class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee129f48-e9b4-4654-8a41-58edd6e8e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNode:\n",
    "    def __init__(self, data, p = None, l = None, r = None, feat = None, threshold = None):\n",
    "        self.data = data\n",
    "        self.l = l\n",
    "        self.r = r\n",
    "        self.p = p\n",
    "        self.feat = feat\n",
    "        self.threshold = threshold\n",
    "        self.predicted_class = None\n",
    "        self.confidence = None\n",
    "    def get_split_data(self):\n",
    "        # splits data according to the specified feature and threshold\n",
    "        data_l = self.data[self.data[self.feat] < self.threshold]\n",
    "        data_r = self.data[self.data[self.feat] >= self.threshold]\n",
    "        return data_l, data_r\n",
    "    def get_gini(self): # gini impurity of the dataset contained in the node\n",
    "        y = self.data.iloc[:,-1]\n",
    "        # print(y)\n",
    "        class_counts = y.value_counts()\n",
    "        probs = class_counts / len(y)\n",
    "        gini = 1- sum(probs*probs)\n",
    "        return gini\n",
    "    def get_weighted_gini(self): #gini weighted on sample size, useful for calculation\n",
    "        y = self.data.iloc[:,-1]\n",
    "        class_counts = y.value_counts()\n",
    "        probs = class_counts / len(y)\n",
    "        gini = 1- sum(probs**2)\n",
    "        return gini * self.get_data_size()\n",
    "    def get_data_size(self):\n",
    "        return self.data.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204815e2-6a62-49f0-a507-e910188868c2",
   "metadata": {},
   "source": [
    "define DT class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3346d36a-8246-4597-9322-0b370a157949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DT:\n",
    "    def __init__(self, feature_list, max_depth = 64, min_split = 3, min_gini = 0.02):\n",
    "        #self.find_best_split(self.root)\n",
    "        self.feature_list = feature_list\n",
    "        self.max_depth= max_depth\n",
    "        self.min_split = min_split\n",
    "        self.min_gini = min_gini\n",
    "    def fit(self, data):\n",
    "        self.root = DTNode(data)\n",
    "        self.build_tree(self.root,1)\n",
    "    def predict(self, data):\n",
    "        # returns a list of expected target values for the input dataset\n",
    "        prediction_list = []\n",
    "        for row_pos in range(data.shape[0]):\n",
    "            prediction_list.append(self.predict_row(data.iloc[row_pos], self.root))\n",
    "        return prediction_list\n",
    "    def predict_row(self, row, node):\n",
    "        if node.l is None and node.r is None:\n",
    "            return node.predicted_class # if node is leaf node, return prediction based on most common target value\n",
    "        else:\n",
    "            if row[node.feat] < node.threshold: # recursively traverse the tree until a leaf node\n",
    "                return self.predict_row(row, node.l) if node.l is not None else self.predict_row(row, node.r)\n",
    "            else:\n",
    "                return self.predict_row(row, node.r) if node.r is not None else self.predict_row(row, node.l)\n",
    "    def build_tree(self, node, depth):\n",
    "        if node.get_data_size() <= self.min_split or depth > self.max_depth or node.get_gini() <= self.min_gini:\n",
    "            node.l = None\n",
    "            node.r = None\n",
    "            y = node.data.iloc[:,-1]\n",
    "            node.predicted_class = y.mode()[0]\n",
    "            return\n",
    "        else:\n",
    "            best_feature, best_threshold = self.find_best_split(node, self.feature_list)\n",
    "            l,r = self.split_node(node,best_feature,best_threshold)\n",
    "            self.build_tree(l, depth + 1)\n",
    "            self.build_tree(r, depth + 1)\n",
    "            node.l = l\n",
    "            node.r = r\n",
    "            return \n",
    "    def find_best_split(self, node, feature_list):\n",
    "        best_cost = 99999999\n",
    "        for feature in feature_list:\n",
    "            for threshold in np.arange(node.data[feature].min(), node.data[feature].max(), 0.5):\n",
    "                temp_node = DTNode(node.data,feat = feature,threshold = threshold)\n",
    "                temp_node.l = DTNode(temp_node.get_split_data()[0])\n",
    "                temp_node.r = DTNode(temp_node.get_split_data()[1])\n",
    "                # calculate weighted average of gini impurities of both children as the cost\n",
    "                cost = (temp_node.l.get_weighted_gini() + temp_node.r.get_weighted_gini()) / (temp_node.get_data_size())\n",
    "                if cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        return best_feature,best_threshold\n",
    "    def split_node(self,node,feat,threshold):\n",
    "        node.feat = feat\n",
    "        node.threshold = threshold\n",
    "        split_data_l, split_data_r = node.get_split_data()\n",
    "        l = DTNode(split_data_l) if split_data_l.shape[0] > 0 else None\n",
    "        r = DTNode(split_data_r) if split_data_r.shape[0] > 0 else None\n",
    "        return l, r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ff784-1558-47b5-b655-497b9b41c79a",
   "metadata": {},
   "source": [
    "define accuracy evaluation and validation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f59d5e-982d-4566-94c6-5cc9ed927fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(training_data,test_data,feature_list, max_depth = 64, min_split = 1, min_gini = 0.05):\n",
    "    correct_predictions = 0\n",
    "    TestTree = DT(feature_list = feature_list, max_depth = max_depth, min_split=min_split, min_gini=min_gini)\n",
    "    TestTree.fit(training_data)\n",
    "    prediction_list = TestTree.predict(test_data)\n",
    "    actual_list = test_data.iloc[:,-1].to_list()\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] == actual_list[i]:\n",
    "            correct_predictions += 1\n",
    "    return correct_predictions / len(prediction_list)\n",
    "\n",
    "def validate(training_data,validation_data,feature_list): # due to the high number of hyperparameters, this can be very slow. min_gini not validated and output not printed for brevity.\n",
    "    best_acc = 0\n",
    "    for max_depth in range(1,15):\n",
    "        for min_split in range(1,7):\n",
    "            # for min_gini in np.arange(0.0,0.2,0.02):\n",
    "            acc = eval_acc(training_data,validation_data,feature_list,max_depth,min_split,0.08)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_depth = max_depth\n",
    "                best_split = min_split\n",
    "                best_gini = 0.08\n",
    "    return best_depth,best_split,best_gini\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec513bd-6597-4aa2-9fb2-c11473fb393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hps for dataset 2: (4, 1, 0.08)\n",
      "Has an accuracy of: 0.9261744966442953\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters_2 = validate(df2_train,df2_validate,feature_list_2)\n",
    "print(\"Best hps for dataset 2:\", best_hyperparameters_2)\n",
    "print(\"Has an accuracy of:\", eval_acc(df2_train,df2_test,feature_list_2,best_hyperparameters_2[0],best_hyperparameters_2[1],best_hyperparameters_2[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad43106-0f76-480e-96b2-c087ac2422d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters_1 = validate(df1_train,df1_validate,feature_list)\n",
    "print(\"Best hps for dataset 1:\", best_hyperparameters_1)\n",
    "print(\"Has an accuracy of:\", eval_acc(df1_train,df1_test,feature_list,best_hyperparameters_1[0],best_hyperparameters_1[1],best_hyperparameters_1[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
