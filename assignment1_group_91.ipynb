{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "AQ4O7ME3e7bJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "jxw1CYU2YKmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9lPMwC3UWjQD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Set the random seed\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the device into GPU"
      ],
      "metadata": {
        "id": "MtYYpBKsYtGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "print(f\"Device using: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKt93gSuYzGo",
        "outputId": "b7d28d76-1d69-4d06-ce2f-6d11b827a922"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the ucimlrepo package"
      ],
      "metadata": {
        "id": "alNW9B1zdjzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install ucimlrepo"
      ],
      "metadata": {
        "id": "55SqN6DKdsSv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataframe 1 (df1)\n",
        "\n",
        "*   Shape of X: (2278, 10)\n",
        "*   Shape of y: (2278, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "P3XtaLDLZdJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 1: NHANES age prediction.csv\n",
        "#(National Health and Nutrition Health Sur- vey 2013-2014 (NHANES) Age Prediction Subset)\n",
        "df1 = pd.read_csv('/content/NHANES_age_prediction.csv') # Change it to wherever you store your dataset"
      ],
      "metadata": {
        "id": "caxbc0cnaZsu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset 2 (df2)\n",
        "Please uncomment it to use this dataset\n",
        "\n",
        "*   Shape of X: (699, 9)\n",
        "*   Shape of y: (699, 1)"
      ],
      "metadata": {
        "id": "AQ4O7ME3e7bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# # fetch dataset\n",
        "# dataset2 = fetch_ucirepo(id=15)\n",
        "\n",
        "# df2 = dataset2.data\n",
        "\n",
        "# # data (as pandas dataframes)\n",
        "# X = dataset2.data.features\n",
        "# y = dataset2.data.targets\n",
        "\n",
        "# # metadata\n",
        "# print(dataset2.metadata)\n",
        "\n",
        "# # variable information\n",
        "# print(dataset2.variables)"
      ],
      "metadata": {
        "id": "DyJUoy82e91Y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the dataset 1\n"
      ],
      "metadata": {
        "id": "ulpJIECVhI2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic information of df1\n",
        "df1.info()\n",
        "\n",
        "# Clean the dataset\n",
        "df1.isnull().sum()\n",
        "print(\"Note: There is no missing value.\\n\")\n",
        "\n",
        "# Drop duliplicate\n",
        "df1.drop_duplicates(inplace=True)\n",
        "\n",
        "# As this task is for classification the input feature and the Target variable will be as follows:\n",
        "# Input Feature : ['RIAGENDR', 'PAQ605', 'BMXBMI', 'LBXGLU', 'DIQ010', 'LBXGLT', 'LBXIN']\n",
        "# Target : ['age_group']\n",
        "X = df1[['RIAGENDR', 'PAQ605', 'BMXBMI', 'LBXGLU', 'DIQ010', 'LBXGLT', 'LBXIN']].values\n",
        "y = pd.get_dummies(df1['age_group']).values # One-hot encoding (Change the categorial y into integer array)\n",
        "\n",
        "# .values: Change the panda dataframe to numpy array\n",
        "\n",
        "# Print the feature shape and classes of dataset\n",
        "(N,D), C = X.shape, np.unique(y)\n",
        "print(f'instances (N) \\t {N} \\n features (D) \\t {D} \\n classes (C) \\t {C}')\n",
        "\n",
        "\n",
        "#generates an indices array from 0 to N-1 and permutes it\n",
        "inds = np.random.permutation(N)\n",
        "\n",
        "train_split, validate_split, test_split = 0.33, 0.33, 0.33\n",
        "\n",
        "# Calculate the indices for each split\n",
        "train_end = int(len(X) * train_split)\n",
        "validate_end = int(len(X) * (train_split + validate_split))\n",
        "\n",
        "# Split the data\n",
        "x_train, y_train = X[inds[:train_end]], y[inds[:train_end]]\n",
        "x_validate, y_validate = X[inds[train_end:validate_end]], y[inds[train_end:validate_end]]\n",
        "x_test, y_test = X[inds[validate_end:]], y[inds[validate_end:]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGJBiK7nejSu",
        "outputId": "de526cf0-691f-4064-aed2-65b14dd906f7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2278 entries, 0 to 2277\n",
            "Data columns (total 10 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   SEQN       2278 non-null   float64\n",
            " 1   age_group  2278 non-null   object \n",
            " 2   RIDAGEYR   2278 non-null   float64\n",
            " 3   RIAGENDR   2278 non-null   float64\n",
            " 4   PAQ605     2278 non-null   float64\n",
            " 5   BMXBMI     2278 non-null   float64\n",
            " 6   LBXGLU     2278 non-null   float64\n",
            " 7   DIQ010     2278 non-null   float64\n",
            " 8   LBXGLT     2278 non-null   float64\n",
            " 9   LBXIN      2278 non-null   float64\n",
            "dtypes: float64(9), object(1)\n",
            "memory usage: 178.1+ KB\n",
            "Note: There is no missing value.\n",
            "\n",
            "instances (N) \t 2278 \n",
            " features (D) \t 7 \n",
            " classes (C) \t [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN model"
      ],
      "metadata": {
        "id": "TiMogxf97b84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "    def __init__(self, K, distance_fn):\n",
        "        self.K = K\n",
        "        self.distance_fn = distance_fn\n",
        "        return\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "        # Number of labels\n",
        "        self.C = len(np.unique(y))\n",
        "        return self\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        # Calculate distances using the distance function\n",
        "        distances = self.distance_fn(self.x[None,:,:], x_test[:,None,:])\n",
        "\n",
        "        num_test = len(x_test)\n",
        "        # Stores the indices of k closest training samples to each test sample\n",
        "        knns = np.zeros((num_test, self.K), dtype=int)\n",
        "        # Stores the probability distribution over C classes\n",
        "        y_prob = np.zeros((num_test, self.C))\n",
        "\n",
        "        for i in range(num_test):\n",
        "            knn_indices = np.argsort(distances[i])[:self.K]\n",
        "            for k in knn_indices:\n",
        "                neighbor_label_vector = self.y[k]\n",
        "                weight = 1 / (distances[i][k] + 1e-5)  # inverse distance as weight\n",
        "                y_prob[i] += weight * neighbor_label_vector\n",
        "\n",
        "        y_pred = np.argmax(y_prob, axis=1)\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "ADLslihw7d_I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose the best hyperparameter K"
      ],
      "metadata": {
        "id": "jCBuoFLRA5bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_k = None\n",
        "best_accuracy = 0\n",
        "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
        "\n",
        "# Convert y_validate from one-hot encoding to class indices if necessary\n",
        "y_validate_indices = np.argmax(y_validate, axis=1)\n",
        "\n",
        "# Try different values of K\n",
        "for K in range(1, 30):  # Assuming we are testing K from 1 to 19\n",
        "    model = KNN(K=K, distance_fn=euclidean)\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_validate)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.sum(y_pred == y_validate_indices) / len(y_validate_indices)\n",
        "    print(f'K = {K}, Validation Accuracy = {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Update best K if current accuracy is better\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_k = K\n",
        "\n",
        "print(f'Best K: {best_k} with Validation Accuracy: {best_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IC1KBkwA44E",
        "outputId": "8a6b9a1a-389d-4457-dee7-b94a1d5c2404"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K = 1, Validation Accuracy = 75.66%\n",
            "K = 2, Validation Accuracy = 75.66%\n",
            "K = 3, Validation Accuracy = 80.19%\n",
            "K = 4, Validation Accuracy = 80.98%\n",
            "K = 5, Validation Accuracy = 82.58%\n",
            "K = 6, Validation Accuracy = 82.18%\n",
            "K = 7, Validation Accuracy = 82.18%\n",
            "K = 8, Validation Accuracy = 83.51%\n",
            "K = 9, Validation Accuracy = 83.24%\n",
            "K = 10, Validation Accuracy = 83.51%\n",
            "K = 11, Validation Accuracy = 83.78%\n",
            "K = 12, Validation Accuracy = 83.24%\n",
            "K = 13, Validation Accuracy = 83.78%\n",
            "K = 14, Validation Accuracy = 83.91%\n",
            "K = 15, Validation Accuracy = 84.31%\n",
            "K = 16, Validation Accuracy = 84.18%\n",
            "K = 17, Validation Accuracy = 84.31%\n",
            "K = 18, Validation Accuracy = 83.91%\n",
            "K = 19, Validation Accuracy = 84.44%\n",
            "K = 20, Validation Accuracy = 84.04%\n",
            "K = 21, Validation Accuracy = 84.04%\n",
            "K = 22, Validation Accuracy = 83.64%\n",
            "K = 23, Validation Accuracy = 84.04%\n",
            "K = 24, Validation Accuracy = 83.64%\n",
            "K = 25, Validation Accuracy = 84.04%\n",
            "K = 26, Validation Accuracy = 84.18%\n",
            "K = 27, Validation Accuracy = 84.31%\n",
            "K = 28, Validation Accuracy = 83.91%\n",
            "K = 29, Validation Accuracy = 84.18%\n",
            "Best K: 19 with Validation Accuracy: 84.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the final evaluation metric"
      ],
      "metadata": {
        "id": "r0YmPWkZBEmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing the best K according to the validation set above\n",
        "myK = 19\n",
        "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
        "\n",
        "model = KNN(K=myK, distance_fn=euclidean)\n",
        "\n",
        "y_pred = model.fit(x_train, y_train).predict(x_test)\n",
        "\n",
        "# This step is converting y_test from one-hot encoding back to class index\n",
        "y_test_indices = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = np.sum(y_pred == y_test_indices)/y_test_indices.shape[0]\n",
        "print(f'accuracy is {accuracy*100:.1f}.')\n",
        "\n"
      ],
      "metadata": {
        "id": "mkknZmXagL_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3a740a-5623-4d37-8835-134726894878"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy is 82.5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxJ-hSe9CKBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fc33be-5bba-4bd2-b348-7e886452e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbe8048-c2d7-40a0-a33e-6a672cc60f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0acc31-5b77-4433-babd-47f341550916",
   "metadata": {},
   "source": [
    "fetch, clean up and separate datasets into training, validation, and testing sets at a ratio of 1:1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fa575e-fd26-4c91-a22f-05dc8d51b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = fetch_ucirepo(id=887)\n",
    "X = dataset1.data.features \n",
    "y = dataset1.data.targets\n",
    "df1 = pd.concat([X,y], axis = 1)\n",
    "df1 = df1.dropna()\n",
    "df1 = df1.drop_duplicates()\n",
    "temp = np.array_split(df1, 3)\n",
    "df1_train = temp[2]\n",
    "df1_validate = temp[1]\n",
    "df1_test = temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c773db84-7522-48d6-9576-fdc0052d34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = fetch_ucirepo(id=15)\n",
    "X2 = dataset2.data.features\n",
    "y2 = dataset2.data.targets\n",
    "df2 = pd.concat([X2,y2], axis = 1)\n",
    "df2 = df2.drop_duplicates()\n",
    "df2 = df2.dropna()\n",
    "temp2 = np.array_split(df2, 3)\n",
    "df2_train = temp2[0]\n",
    "df2_validate = temp2[1]\n",
    "df2_test = temp2[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addd85f-a4a7-410f-aeb0-2696d8c9874f",
   "metadata": {},
   "source": [
    "get a list of feature names for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd12644-4323-4394-95fd-275746a2119d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = df1.columns.tolist()\n",
    "column_list = feature_list[:]\n",
    "feature_list.pop()\n",
    "\n",
    "feature_list_2 = df2.columns.tolist()\n",
    "column_list_2 = feature_list_2[:]\n",
    "feature_list_2.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec4f1279-9270-4ad7-a871-06d899955ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RIAGENDR', 'PAQ605', 'BMXBMI', 'LBXGLU', 'DIQ010', 'LBXGLT', 'LBXIN'] ['Clump_thickness', 'Uniformity_of_cell_size', 'Uniformity_of_cell_shape', 'Marginal_adhesion', 'Single_epithelial_cell_size', 'Bare_nuclei', 'Bland_chromatin', 'Normal_nucleoli', 'Mitoses']\n"
     ]
    }
   ],
   "source": [
    "print(feature_list, feature_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa011c0a-ee83-40a0-b935-0607632255fc",
   "metadata": {},
   "source": [
    "define DTNode class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee129f48-e9b4-4654-8a41-58edd6e8e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNode:\n",
    "    def __init__(self, data, p = None, l = None, r = None, feat = None, threshold = None):\n",
    "        self.data = data\n",
    "        self.l = l\n",
    "        self.r = r\n",
    "        self.p = p\n",
    "        self.feat = feat\n",
    "        self.threshold = threshold\n",
    "        self.predicted_class = None\n",
    "        self.confidence = None\n",
    "    def get_split_data(self):\n",
    "        # splits data according to the specified feature and threshold\n",
    "        data_l = self.data[self.data[self.feat] < self.threshold]\n",
    "        data_r = self.data[self.data[self.feat] >= self.threshold]\n",
    "        return data_l, data_r\n",
    "    def get_gini(self): # gini impurity of the dataset contained in the node\n",
    "        y = self.data.iloc[:,-1]\n",
    "        # print(y)\n",
    "        class_counts = y.value_counts()\n",
    "        probs = class_counts / len(y)\n",
    "        gini = 1- sum(probs*probs)\n",
    "        return gini\n",
    "    def get_weighted_gini(self): #gini weighted on sample size, useful for calculation\n",
    "        y = self.data.iloc[:,-1]\n",
    "        class_counts = y.value_counts()\n",
    "        probs = class_counts / len(y)\n",
    "        gini = 1- sum(probs**2)\n",
    "        return gini * self.get_data_size()\n",
    "    def get_data_size(self):\n",
    "        return self.data.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204815e2-6a62-49f0-a507-e910188868c2",
   "metadata": {},
   "source": [
    "define DT class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3346d36a-8246-4597-9322-0b370a157949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DT:\n",
    "    def __init__(self, feature_list, max_depth = 64, min_split = 3, min_gini = 0.02):\n",
    "        #self.find_best_split(self.root)\n",
    "        self.feature_list = feature_list\n",
    "        self.max_depth= max_depth\n",
    "        self.min_split = min_split\n",
    "        self.min_gini = min_gini\n",
    "    def fit(self, data):\n",
    "        self.root = DTNode(data)\n",
    "        self.build_tree(self.root,1)\n",
    "    def predict(self, data):\n",
    "        # returns a list of expected target values for the input dataset\n",
    "        prediction_list = []\n",
    "        for row_pos in range(data.shape[0]):\n",
    "            prediction_list.append(self.predict_row(data.iloc[row_pos], self.root))\n",
    "        return prediction_list\n",
    "    def predict_row(self, row, node):\n",
    "        if node.l is None and node.r is None:\n",
    "            return node.predicted_class # if node is leaf node, return prediction based on most common target value\n",
    "        else:\n",
    "            if row[node.feat] < node.threshold: # recursively traverse the tree until a leaf node\n",
    "                return self.predict_row(row, node.l) if node.l is not None else self.predict_row(row, node.r)\n",
    "            else:\n",
    "                return self.predict_row(row, node.r) if node.r is not None else self.predict_row(row, node.l)\n",
    "    def build_tree(self, node, depth):\n",
    "        if node.get_data_size() <= self.min_split or depth > self.max_depth or node.get_gini() <= self.min_gini:\n",
    "            node.l = None\n",
    "            node.r = None\n",
    "            y = node.data.iloc[:,-1]\n",
    "            node.predicted_class = y.mode()[0]\n",
    "            return\n",
    "        else:\n",
    "            best_feature, best_threshold = self.find_best_split(node, self.feature_list)\n",
    "            l,r = self.split_node(node,best_feature,best_threshold)\n",
    "            self.build_tree(l, depth + 1)\n",
    "            self.build_tree(r, depth + 1)\n",
    "            node.l = l\n",
    "            node.r = r\n",
    "            return \n",
    "    def find_best_split(self, node, feature_list):\n",
    "        best_cost = 99999999\n",
    "        for feature in feature_list:\n",
    "            for threshold in np.arange(node.data[feature].min(), node.data[feature].max(), 0.5):\n",
    "                temp_node = DTNode(node.data,feat = feature,threshold = threshold)\n",
    "                temp_node.l = DTNode(temp_node.get_split_data()[0])\n",
    "                temp_node.r = DTNode(temp_node.get_split_data()[1])\n",
    "                # calculate weighted average of gini impurities of both children as the cost\n",
    "                cost = (temp_node.l.get_weighted_gini() + temp_node.r.get_weighted_gini()) / (temp_node.get_data_size())\n",
    "                if cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        return best_feature,best_threshold\n",
    "    def split_node(self,node,feat,threshold):\n",
    "        node.feat = feat\n",
    "        node.threshold = threshold\n",
    "        split_data_l, split_data_r = node.get_split_data()\n",
    "        l = DTNode(split_data_l) if split_data_l.shape[0] > 0 else None\n",
    "        r = DTNode(split_data_r) if split_data_r.shape[0] > 0 else None\n",
    "        return l, r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ff784-1558-47b5-b655-497b9b41c79a",
   "metadata": {},
   "source": [
    "define accuracy evaluation and validation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f59d5e-982d-4566-94c6-5cc9ed927fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(training_data,test_data,feature_list, max_depth = 64, min_split = 1, min_gini = 0.05):\n",
    "    correct_predictions = 0\n",
    "    TestTree = DT(feature_list = feature_list, max_depth = max_depth, min_split=min_split, min_gini=min_gini)\n",
    "    TestTree.fit(training_data)\n",
    "    prediction_list = TestTree.predict(test_data)\n",
    "    actual_list = test_data.iloc[:,-1].to_list()\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] == actual_list[i]:\n",
    "            correct_predictions += 1\n",
    "    return correct_predictions / len(prediction_list)\n",
    "\n",
    "def validate(training_data,validation_data,feature_list): # due to the high number of hyperparameters, this can be very slow. min_gini not validated and output not printed for brevity.\n",
    "    best_acc = 0\n",
    "    for max_depth in range(1,15):\n",
    "        for min_split in range(1,7):\n",
    "            # for min_gini in np.arange(0.0,0.2,0.02):\n",
    "            acc = eval_acc(training_data,validation_data,feature_list,max_depth,min_split,0.08)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_depth = max_depth\n",
    "                best_split = min_split\n",
    "                best_gini = 0.08\n",
    "    return best_depth,best_split,best_gini\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec513bd-6597-4aa2-9fb2-c11473fb393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hps for dataset 2: (4, 1, 0.08)\n",
      "Has an accuracy of: 0.9261744966442953\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters_2 = validate(df2_train,df2_validate,feature_list_2)\n",
    "print(\"Best hps for dataset 2:\", best_hyperparameters_2)\n",
    "print(\"Has an accuracy of:\", eval_acc(df2_train,df2_test,feature_list_2,best_hyperparameters_2[0],best_hyperparameters_2[1],best_hyperparameters_2[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad43106-0f76-480e-96b2-c087ac2422d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters_1 = validate(df1_train,df1_validate,feature_list)\n",
    "print(\"Best hps for dataset 1:\", best_hyperparameters_1)\n",
    "print(\"Has an accuracy of:\", eval_acc(df1_train,df1_test,feature_list,best_hyperparameters_1[0],best_hyperparameters_1[1],best_hyperparameters_1[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
  }
}
